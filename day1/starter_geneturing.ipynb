{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a8b93f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# Starter Notebook for GeneTuring Few-shot Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260d323",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Let's start of by implementing a basic harness to evaluate the Geneturing dataset using Ollama with a Few-Shot prompting strategy to evaluate the performance of these models before implementing the tool-using strategies outlined in the GeneGPT paper.\n",
    "\n",
    "As with most benchmarking code using pretrained models, our notebook will following typical outline of:\n",
    "\n",
    "1. **Imports**\n",
    "2. **Configuration**\n",
    "3. **Data Loading**\n",
    "4. **Model Specification**\n",
    "5. **Metrics**\n",
    "6. **Evaluation Loop**\n",
    "7. **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecb4f5",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db83b1e",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Good hygiene for Jupyter notebooks includes placing all of the imports at the top of the notebook. This makes it easier to understand what dependencies are needed to run the notebook for new users and mirrors good practices for Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb9107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Place imports here\n",
    "from typing import List, Optional\n",
    "\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c15925",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf9042",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Also, let's create a section that is specific to the configuration of the run. This will make it easier to change the configuration of the run without hunting for hard-coded values sprinkled throughout the code and makes it easier for others to understand the configuration of the run.\n",
    "\n",
    "We will leave it empty at the moment, but we will come back and fill it in as we identify global configuration options that we need to implement the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d543d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Data Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e42308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac0d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Evaluation and Logging Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904443",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d7a1f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "In this section we need to load the data that we will be using for our evaluation. \n",
    "\n",
    "The JSON file at `data/gene_turing.json` contains a dictionary of dictionaries. At the top level, the keys are the names of the tasks. Within each task, there are several key-value pairs where the keys are the questions and the values are the answers.\n",
    "\n",
    "Below is one example for each of the 9 tasks in the dataset:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Gene alias\": {\n",
    "        \"What is the official gene symbol of LMP10?\": \"PSMB10\"\n",
    "    },\n",
    "    \"Gene disease association\": {\n",
    "        \"What are genes related to Hemolytic anemia due to phosphofructokinase deficiency?\": \"PFKL\"\n",
    "    },\n",
    "    \"Gene location\": {\n",
    "        \"Which chromosome is FAM66D gene located on human genome?\": \"chr8\"\n",
    "    },\n",
    "    \"Human genome DNA aligment\": {\n",
    "        \"Align the DNA sequence to the human genome:ATTCTGCCTTTAGTAATTTGATGACAGAGACTTCTTGGGAACCACAGCCAGGGAGCCACCCTTTACTCCACCAACAGGTGGCTTATATCCAATCTGAGAAAGAAAGAAAAAAAAAAAAGTATTTCTCT\": \"chr15:91950805-91950932\",\n",
    "    },\n",
    "    \"Multi-species DNA aligment\": {\n",
    "        \"Which organism does the DNA sequence come from:AGGGGCAGCAAACACCGGGACACACCCATTCGTGCACTAATCAGAAACTTTTTTTTCTCAAATAATTCAAACAATCAAAATTGGTTTTTTCGAGCAAGGTGGGAAATTTTTCGAT\": \"worm\",\n",
    "    },\n",
    "    \"Gene name conversion\": {\n",
    "        \"Convert ENSG00000215251 to official gene symbol.\": \"FASTKD5\",\n",
    "    },\n",
    "    \"Protein-coding genes\": {\n",
    "        \"Is ATP5F1EP2 a protein-coding gene?\": \"NA\",\n",
    "    },\n",
    "    \"Gene SNP association\": {\n",
    "        \"Which gene is SNP rs1217074595 associated with?\": \"LINC01270\",\n",
    "    },\n",
    "    \"SNP location\": {\n",
    "        \"Which chromosome does SNP rs1430464868 locate on human genome?\": \"chr13\",\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e217f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "\n",
    "We need to reformat this into a pandas dataframe with the following columns:\n",
    "- `id`: A serial ID number we will assign to each example (int)\n",
    "- `task`: The name of the task (str)\n",
    "- `question`: The question for the example (str)\n",
    "- `answer`: The answer for the example (str)\n",
    "\n",
    "The final dataframe we will create should look like this:\n",
    "\n",
    "| id | task | question | answer |\n",
    "|----|------|----------|--------|\n",
    "| 0 | Task1 | Question1 | Answer1 |\n",
    "| 1 | Task1 | Question2 | Answer2 |\n",
    "| 2 | Task2 | Question1 | Answer1 |\n",
    "| 3 | Task2 | Question2 | Answer2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load the JSON file\n",
    "\n",
    "# Load the data here\n",
    "\n",
    "# Build the TASKS variable here\n",
    "\n",
    "TASKS = set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9a5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Iterate through the JSON data recursively to collect each of the rows into a list\n",
    "#     Each row should have a dictionary with keys of the columsn in the table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Create the pandas dataframe from the collection of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f330e0",
   "metadata": {},
   "source": [
    "## 4. Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dff55",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "With our few-shot in-context learning model, we need to specify four components:\n",
    "\n",
    "1. The large language model to use\n",
    "2. The instructions for the model as a system prompt\n",
    "3. The few-shot examples to provide to the model to demonstrate the input-output format\n",
    "4. The completion request function that puts it all together retrieving a response for each unseen input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a9f18",
   "metadata": {},
   "source": [
    "### 4.1 Setting up the large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f0a94",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "We will use the Ollama client to interface with the large language model on the Ollama server we started. With large language models, it is common to use a client library to interface with the model hosted by a server. This allows us to iterate quickly on the prompting and post-processing logic without having to incur the overhead of loading the model into memory each time.  Additionally, model code is oftentimes optimized for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Setting up the large language model Ollama model client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf45c9",
   "metadata": {},
   "source": [
    "### 4.2 Setting up the system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46af38",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Modern large language models are post-trained to perform a variety of tasks and follow instructions. To leverage this capability, we need to provide a system prompt that clearly outlines the task, any constraints, and the format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bf3d5",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Designing the system prompt is a critical aspect of using LLMs. Below are several resources for designing a system prompt:\n",
    "* [OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/text?api-mode=responses#prompt-engineering)\n",
    "* [Kaggle/Google Prompt Engineering](https://www.kaggle.com/whitepaper-prompt-engineering?_bhlid=a2bfce2cac67662098bd85a241e7cb000576e5d4)\n",
    "* [Anthropic Prompt Engineering](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n",
    "* [OpenAI GPT 4.1 Prompting Cookbook](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887a248",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "**From the OpenAI Prompt Engineering guide:**\n",
    "\n",
    "> **Identity**: Describe the purpose, communication style, and high-level goals of the assistant. \n",
    "> \n",
    "> **Instructions**: Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should call custom functions.  \n",
    ">\n",
    "> **Examples**: Provide examples of possible inputs, along with the desired output from the model.  \n",
    "> \n",
    "> **Context**: Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebc861",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "This is what the system prompt looked like in the originalGeneGPT paper, but it is not the best. Identify what it includes and what is missing. Implement your own system prompt incorporating best practices from some of the guides posted above. \n",
    "\n",
    "> \t'Hello. Your task is to use NCBI Web APIs to answer genomic questions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Draft your own system prompt for our generic genomics question answering system. \n",
    "#     Replace the system message `content` below with your own.\n",
    "system_message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Hello. Your task is to use NCBI Web APIs to answer genomic questions.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0bf3f",
   "metadata": {},
   "source": [
    "### 4.3 Setting up the few-shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8d700",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "For tasks with idiosyncratic output formats or constraints, it is important to provide clear instructions as well as examples to guide the model in generating the desired output. Mechanically, we provide these pairs of inputs and outputs as a sequence of user and assistant messages after the system prompt.\n",
    "\n",
    "```python\n",
    "messages += [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  <fill in input example 1>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": <fill in output example 1>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": <fill in input example 2>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": <fill in output example 2>\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc96d8",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "In the GeneGPT code, the authors included several tasks, one of each of a subset of the tasks in the dataset. We will use the same examples here.\n",
    "\n",
    "Please inspect the GeneGPT repository to find the few-shot examples in the prompt. \n",
    "\n",
    "Specifically the `get_prompt_header` function in `main.py` located here: [main.py](https://github.com/ncbi/GeneGPT/blob/main/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb78ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Appending the few-shot examples to the `messages` list\n",
    "example_messages = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d89761",
   "metadata": {},
   "source": [
    "### 4.4 Implementing the model request function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590511a",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now we need to put it all together. We need a function that accepts as arguments:\n",
    "1. The client\n",
    "2. The system message\n",
    "3. The few-shot examples\n",
    "4. The new user query -- this case the user's question from the GeneTuring dataset.\n",
    "\n",
    "The function should return the response from the model and extract the answer (everything after 'Answer :' based on the format of the examples above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Implement THE model function\n",
    "def query_model(\n",
    "    client: OllamaClient, \n",
    "    system_message: dict, \n",
    "    few_shot_examples: List[dict], \n",
    "    user_query: str\n",
    ") -> str:\n",
    "    \"\"\"Implement ME\"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103ef75",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d88fd6",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "The GeneTuring dataset has several task specific evaluation metrics that are variations on exact match depending on the expected cardinality of the output.\n",
    "\n",
    "Please inspect `evaluate.py` from the GeneGPT repository to find the metric functions and implement them here.\n",
    "\n",
    "Original functions: [evaluate.py](https://github.com/ncbi/GeneGPT/blob/main/evaluate.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc440b3",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "* **Default exact match** - The predicted answers and ground truth are both strings. The score is 1 if they are equal and 0 otherwise\n",
    "* **Gene disease association** - The predicted answers and ground truth are both lists of gene-disease associations. The score is the proportion of correct associations present in the prediction\n",
    "* **Disease gene location** - The predicted and true answers are lists (e.g., gene locations related to a disease), and the evaluation calculates the fraction of the correct items present in the prediction.\n",
    "* **Human genome DNA aligment** - 1 point for exact match, 0.5 point if chrX part matches, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement metrics\n",
    "\n",
    "def exact_match(pred: str, true: str) -> float:\n",
    "    raise NotImplementedError\n",
    "\n",
    "def gene_disease_association(pred: list, true: list) -> float:\n",
    "    raise NotImplementedError\n",
    "\n",
    "def disease_gene_location(pred: list, true: list) -> float:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def human_genome_dna_alignment(pred: str, true: str) -> float:\n",
    "    raise NotImplementedError\n",
    "\n",
    "metric_task_map = defaultdict(exact_match, {\n",
    "    \"gene_disease_association\": gene_disease_association,\n",
    "    \"disease_gene_location\": disease_gene_location,\n",
    "    \"human_genome_dna_alignment\": human_genome_dna_alignment\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ebe36",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Many of the gold-answers are in a specific format. `evaluate.py` also implements an answer post-processing function `get_answer` to better align model outputs with the gold answers. We also need to implement a similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bd36ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Implement the answer mapping function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e73a3",
   "metadata": {},
   "source": [
    "## 6. Evaluation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aba90",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now, let's implement the evaluation loop for the GeneTuring dataset. For now we will call the model function one at a time and collect the results in a list. Also, we will collect per-task metrics and the overall metrics for the dataset as we go. Once we're done, we will save the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78868c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Set up data structures for results\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    id: int\n",
    "    task: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    raw_prediction: Optional[str]\n",
    "    processed_prediction: Optional[str]\n",
    "    score: Optional[float]\n",
    "    success: bool\n",
    "\n",
    "def save_results(results: List[Result], results_csv_filename: str) -> None:\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56608117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Loop over the dataset with a progress bar\n",
    "\n",
    "# * Do not forget to add the results to our Result list, both successful and failed predictions\n",
    "# * API calls will not always work, so make sure we capture the exceptions from failed calls\n",
    "#    and add them to the Result list with a `status=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cc013b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dab8dd",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4efaa",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now that we have collected the first round of GeneTuring results, let's analyze them. Let's start by calculating what fraction of predictions were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcca4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Calculate the fraction of successful predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a3607",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now let's calculate both the overall score as well as the score by task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f7c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Calculate the overall score and the score by task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc369e",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Then, let's create a bar chart of the scores by task with a horizontal line for the overall score. Let's save the figure as well to our output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Create a bar chart of the scores by task with a horizontal line for the overall score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
